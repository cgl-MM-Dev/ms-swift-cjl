# Swift SFT 完整训练配置文件 - 支持 Wandb 和环境变量
# 基于官方文档的所有参数，包含详细注释和候选值

# ==============================
# 🔥 基本参数 (Basic Parameters)
# ==============================

tuner_backend: peft  # 可选: 'peft', 'unsloth'。默认为'peft'
train_type: full     # 可选: 'lora', 'full', 'longlora', 'adalora', 'llamapro', 'adapter', 'vera', 'boft', 'fourierft', 'reft'
adapters: []         # 适配器路径列表，默认为[]
external_plugins: [] # 外部插件文件列表，默认为[]
seed: 42            # 随机种子，默认为42

# ==============================
# 🔥 模型参数 (Model Parameters)
# ==============================

model: /workspace/ckpt-tmp-1/Qwen2_5-VL-7B-Instruct
model_type: qwen2_5_vl  # 常见: qwen2_5_vl, qwen2_vl, glm4v, internvl2, llava
torch_dtype: bfloat16   # 可选: 'float16', 'bfloat16', 'float32'
attn_impl: flash_attn   # 可选: 'sdpa', 'eager', 'flash_attention_2', 'flash_attn'

# ==============================
# 🔥 数据参数 (Data Parameters)
# ==============================

dataset: /workspace/Dataset/v0a/v0a.yaml
val_dataset: []                 # 验证集，默认为[]
split_dataset_ratio: 0.         # 验证集分割比例
dataset_num_proc: 4             # 数据预处理进程数
load_from_cache_file: true      # 是否使用缓存
dataset_shuffle: true           # 训练集打乱
# val_dataset_shuffle: false    # 验证集打乱
remove_unused_columns: true     # 删除未使用的列

# ==============================
# 🔥 训练参数 (Training Parameters)
# ==============================

num_train_epochs: 3                # 训练轮数
per_device_train_batch_size: 1     # 每设备训练批次大小
per_device_eval_batch_size: 1      # 每设备验证批次大小
learning_rate: 1e-6                # 学习率
gradient_accumulation_steps: 8     # 梯度累积步数
warmup_ratio: 0.05                 # 预热比例
dataloader_num_workers: 4          # 数据加载进程数

# 优化器设置
optim: adamw_torch              # 可选: 'adamw_torch', 'adamw_hf', 'sgd'
lr_scheduler_type: cosine       # 可选: 'linear', 'cosine', 'polynomial'
# lr_scheduler_kwargs:
#   min_lr: 5e-6                # transformers 的 cosine scheduler 不支持此参数
weight_decay: 0.01              # 权重衰减
max_grad_norm: 1.0              # 梯度裁剪

# ==============================
# 🔥 LoRA参数 (LoRA Parameters)
# ==============================
lora_configs:
  lora_rank: 8                    # LoRA rank
  lora_alpha: 32                  # LoRA alpha
  lora_dropout: 0.05              # LoRA dropout
  target_modules: all-linear      # 目标模块
  use_dora: false                 # 是否使用DoRA
  use_rslora: false               # 是否使用RS-LoRA
  lorap_lr_ratio: null            # LoRA+参数，建议10~16

# ==============================
# 🔥 多模态参数 (Multimodal Parameters)
# ==============================

max_length: 24576               # 最大序列长度
max_pixels: 200704              # 最大图像像素数
freeze_vit: true                # 冻结视觉编码器
padding_free: false             # 无填充训练

# ==============================
# 🔥 模板参数 (Template Parameters)
# ==============================

template: null                  # 对话模板，默认自动选择
system: null                    # 自定义system提示
truncation_strategy: delete     # 截断策略: 'delete', 'left', 'right'
use_chat_template: true         # 使用chat模板
template_backend: swift         # 模板后端: 'swift', 'jinja'

# ==============================
# 🔥 损失函数 (Loss Parameters)
# ==============================

# loss_type: default              # 可选: 'None', 'graph_loss', 'focal_loss'
# loss_scale: default             # 损失权重: 'last_round', 'all'

# ==============================
# 🔥 输出和日志 (Output & Logging)
# ==============================

output_dir: ./output
logging_steps: 5                # 日志记录间隔
# eval_steps: 500               # 评估间隔
save_strategy: epoch            # 保存的策略 'no'、'steps'、'epoch'，默认为'steps'
# save_steps: 1000                # 保存间隔
save_only_model: true           # 只保存模型权重
save_total_limit: 5             # 保存数量限制
report_to: wandb                # 日志后端: 'tensorboard', 'wandb', 'none'

# ==============================
# 🔥 Wandb配置 (Wandb Configuration)
# ==============================

wandb_project: cjl_sft_tmp-2        # Wandb项目名 (必填)
wandb_run_name: qwen2_5-vl-7b-full-lr-1e-6-v0a-tmp     # 实验运行名称 (可选，默认自动生成)
wandb_tags: ["qwen2.5-vl", "lora", "sft", "complete-config"]  # 实验标签

# 常用wandb配置示例:
# wandb_project: "my_multimodal_sft"
# wandb_run_name: "qwen2_5_vl_lora_rank16_lr2e5"
# wandb_tags: ["multimodal", "vision-language", "fine-tuning"]

# ==============================
# 🔥 DeepSpeed配置 (DeepSpeed)
# ==============================

deepspeed: zero2                # 可选: 'zero1', 'zero2', 'zero3', JSON文件路径

# ==============================
# 🔥 特定模型参数 (Model-specific Parameters)
# ==============================

model_kwargs:
#   # Qwen2.5-VL 特定参数
#   # fps_max_frames: 768           # 视频最大帧数
#   # fps_min_frames: 4             # 视频最小帧数
#   # video_max_pixels: 128000      # 视频最大像素数
#   # min_pixels: 256               # 图片最小像素数
max_pixels: 12845056  # 16384 * 28 * 28     # 图片最大像素数

# 其他模型的 model_kwargs 示例:
# GLM-4V:
# model_kwargs:
#   max_length: 8192
#   temperature: 0.8
#
# InternVL2:
# model_kwargs:
#   dynamic_image_size: true
#   use_thumbnail: true

# ==============================
# 🔥 环境变量配置 (Environment Variables)
# ==============================

env_vars:
  # Wandb 相关环境变量
  WANDB_BASE_URL: "https://wandb.glm.ai/"
  WANDB_API_KEY: "local-d70dccc058669955f0ae6b6d655fbb52b9a994ee"
  WANDB_ENTITY: "garygedegege"
  
  # GPU 设置 (可选，也可以通过命令行设置)
  CUDA_VISIBLE_DEVICES: "4,5,6,7"      # 使用前4张卡
  NPROC_PER_NODE: "4"                  # 每个节点4个进程
  
  # 训练优化相关环境变量
  CUDA_DEVICE_MAX_CONNECTIONS: "1"
  PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
  PYTHONWARNINGS: "ignore"
  # Qwen2.5-VL 特定参数 (通过环境变量传递，避免 JSON 问题)
  # MAX_PIXELS: "12845056"               # 图片最大像素数
  
  # 缓存相关 (可选)
  # HF_HUB_CACHE: "/path/to/hf_cache"
  # TRANSFORMERS_CACHE: "/path/to/transformers_cache"
  
  # 重要说明：模型特定参数 (如 Qwen2.5-VL 的 fps_max_frames, video_max_pixels)
  # 请使用上面的 model_kwargs 字段设置，不要在环境变量中重复设置
  # 
  # 区别说明：
  # - model_kwargs: 直接传递给模型构造函数的参数 (推荐)
  # - 环境变量: 被某些模型代码在运行时读取的系统级设置
  # 
  # 对于 Qwen2.5-VL，两种方式都支持，但我们统一使用 model_kwargs 方式

# 环境变量说明:
# - 所有值必须是字符串格式
# - 环境变量会在训练开始前设置
# - 优先级: env_vars > 脚本默认值 > 系统环境变量
# - 支持覆盖 model_kwargs 中的参数

# ==============================
# 🔥 高级参数 (Advanced Parameters)
# ==============================

# 数据相关
streaming: false                # 流式数据集
packing: false                  # 数据packing
lazy_tokenize: null             # 延迟tokenize
cached_dataset: []              # 缓存数据集路径

# 训练策略
fp16: false                     # 使用FP16
bf16: true                      # 使用BF16 (根据torch_dtype自动设置)
tf32: null                      # 使用TF32

# 检查点相关
resume_from_checkpoint: null    # 恢复训练的检查点
load_best_model_at_end: false   # 加载最佳模型
create_checkpoint_symlink: false # 创建软链接

# 预测和生成
predict_with_generate: false    # 验证时使用生成模式

# 全参数训练相关
freeze_parameters: []           # 冻结参数前缀
freeze_parameters_regex: null   # 冻结参数正则
trainable_parameters: []       # 可训练参数前缀

# Hub相关
push_to_hub: false              # 推送到Hub
hub_model_id: null              # Hub模型ID
use_hf: false                   # 使用HuggingFace

# 其他
ddp_timeout: 18000000           # DDP超时时间
ddp_backend: null               # DDP后端
ignore_args_error: false       # 忽略参数错误

# ==============================
# 使用说明和示例
# ==============================

# 这是一个包含所有参数的完整配置文件模板
# 
# 快速开始:
# 1. 修改 model 路径为你的模型路径
# 2. 修改 dataset 路径为你的数据集路径
# 3. 修改 output_dir 为你的输出目录
# 4. 设置 wandb 相关参数
# 5. 根据需要调整其他参数
#
# 使用方式:
# bash submit_swift_sft.sh configs/swift_sft_complete.yaml
#
# 参数覆盖示例:
# bash submit_swift_sft.sh configs/swift_sft_complete.yaml --learning_rate 2e-5 --lora_rank 16
#
# 注意事项:
# - 🔥 标记的参数是重要参数，新手用户应重点关注
# - 环境变量通过 env_vars 字段统一管理
# - wandb 配置支持完全自定义项目名和实验名
# - 所有参数都有详细注释和候选值说明
